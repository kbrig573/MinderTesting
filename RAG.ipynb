{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0lF2ukk/86b99fXE7aUu2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kbrig573/MinderTesting/blob/Develop/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZndgAHa2kI_"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-groq langchain-community langchain-huggingface chromadb pypdf sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "RAG (Retrieval Augmented Generation) Chatbot\n",
        "-------------------------------------------\n",
        "This implementation follows the RAG architecture with clear separation of components:\n",
        "1. Client Interface: Handles user interaction\n",
        "2. Vector Database: Stores and retrieves document embeddings\n",
        "3. Framework: Manages the RAG pipeline\n",
        "4. LLM Integration: Handles the language model interaction\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from typing import Dict, List, Tuple\n",
        "from dataclasses import dataclass\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "uizWp9Ss-F6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "HcycY1du-OMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "@dataclass\n",
        "class RAGConfig:\n",
        "    \"\"\"Configuration settings for the RAG system\"\"\"\n",
        "    chunk_size: int = 500\n",
        "    chunk_overlap: int = 150\n",
        "    model_name: str = \"mixtral-8x7b-32768\"\n",
        "    temperature: float = 0.7\n",
        "    max_tokens: int = 4096\n",
        "    top_k_results: int = 4"
      ],
      "metadata": {
        "id": "_HbB0Eo9-YLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"Component 1: Handles document loading and preprocessing\"\"\"\n",
        "    def __init__(self, config: RAGConfig):\n",
        "        self.config = config\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=config.chunk_size,\n",
        "            chunk_overlap=config.chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        "        )\n",
        "\n",
        "    def load_and_split_document(self, pdf_path: str) -> List:\n",
        "        \"\"\"Load PDF and split into chunks\"\"\"\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        documents = loader.load()\n",
        "        return self.text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "exbZz0Ho-gRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "    \"\"\"Component 2: Handles vector database operations\"\"\"\n",
        "    def __init__(self, pdf_path: str):\n",
        "        self.persist_dir = f\"chroma_db_{pdf_path}\"\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            model_kwargs={'device': 'cuda'}\n",
        "        )\n",
        "\n",
        "    def initialize_store(self, documents: List) -> Chroma:\n",
        "        \"\"\"Initialize or load vector store\"\"\"\n",
        "        if os.path.exists(self.persist_dir):\n",
        "            print(\"Loading existing vector store...\")\n",
        "            return Chroma(\n",
        "                persist_directory=self.persist_dir,\n",
        "                embedding_function=self.embeddings\n",
        "            )\n",
        "\n",
        "        print(\"Creating new vector store...\")\n",
        "        db = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=self.persist_dir\n",
        "        )\n",
        "        return db"
      ],
      "metadata": {
        "id": "gGSivxmI-jHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PromptManager:\n",
        "    \"\"\"Component 3: Handles prompt engineering and context formatting\"\"\"\n",
        "    @staticmethod\n",
        "    def create_retrieval_prompt(question: str) -> str:\n",
        "        return f\"\"\"Please analyze the provided context carefully and answer the following question.\n",
        "        If the answer isn't clearly supported by the context, please indicate that.\n",
        "\n",
        "        Question: {question}\n",
        "        \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def format_sources(sources: List) -> str:\n",
        "        formatted_sources = []\n",
        "        for idx, doc in enumerate(sources, 1):\n",
        "            page_info = f\"(Page {doc.metadata.get('page', 'N/A')})\"\n",
        "            source_text = f\"{doc.page_content[:200]}...\"\n",
        "            formatted_sources.append(f\"{idx}. {source_text} {page_info}\")\n",
        "        return \"\\n\\n\".join(formatted_sources)"
      ],
      "metadata": {
        "id": "loUIk2oE-mC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGChatbot:\n",
        "    \"\"\"Main RAG Framework: Orchestrates the entire RAG process\"\"\"\n",
        "    def __init__(self, pdf_path: str, config: RAGConfig = RAGConfig()):\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize components\n",
        "        print(\"1. Initializing document processor...\")\n",
        "        self.doc_processor = DocumentProcessor(config)\n",
        "\n",
        "        print(\"2. Processing document...\")\n",
        "        documents = self.doc_processor.load_and_split_document(pdf_path)\n",
        "\n",
        "        print(\"3. Setting up vector store...\")\n",
        "        self.vector_store = VectorStore(pdf_path)\n",
        "        self.vectordb = self.vector_store.initialize_store(documents)\n",
        "\n",
        "        print(\"4. Setting up LLM and conversation chain...\")\n",
        "        self.setup_llm_chain()\n",
        "\n",
        "        self.prompt_manager = PromptManager()\n",
        "\n",
        "    def setup_llm_chain(self):\n",
        "        \"\"\"Set up the LLM and conversation chain\"\"\"\n",
        "        llm = ChatGroq(\n",
        "            model_name=self.config.model_name,\n",
        "            temperature=self.config.temperature,\n",
        "            max_tokens=self.config.max_tokens\n",
        "        )\n",
        "\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            output_key=\"answer\",\n",
        "            return_messages=True\n",
        "        )\n",
        "\n",
        "        self.chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=self.vectordb.as_retriever(\n",
        "                search_type=\"mmr\",\n",
        "                search_kwargs={\n",
        "                    \"k\": self.config.top_k_results,\n",
        "                    \"fetch_k\": self.config.top_k_results * 2,\n",
        "                    \"lambda_mult\": 0.7\n",
        "                }\n",
        "            ),\n",
        "            memory=memory,\n",
        "            return_source_documents=True,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    def chat(self, question: str) -> Dict:\n",
        "        \"\"\"Process a question through the RAG pipeline\"\"\"\n",
        "        try:\n",
        "            # 1. Format the question with proper prompt\n",
        "            formatted_question = self.prompt_manager.create_retrieval_prompt(question)\n",
        "\n",
        "            # 2. Get response from LLM with retrieved context\n",
        "            result = self.chain.invoke({\"question\": formatted_question})\n",
        "\n",
        "            # 3. Format the response and sources\n",
        "            return {\n",
        "                \"answer\": result[\"answer\"],\n",
        "                \"sources\": self.prompt_manager.format_sources(result[\"source_documents\"])\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error in RAG pipeline: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": \"An error occurred while processing your question.\",\n",
        "                \"sources\": []\n",
        "            }\n",
        "\n"
      ],
      "metadata": {
        "id": "81wPSTuU-owK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Client Interface: Handles user interaction\"\"\"\n",
        "    # Initialize with configuration\n",
        "    config = RAGConfig(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=150,\n",
        "        temperature=0.7,\n",
        "        top_k_results=4\n",
        "    )\n",
        "\n",
        "    # Create chatbot instance\n",
        "    pdf_path = \"/content/1706.03762v7.pdf\"  # Attention is All You Need paper\n",
        "    chatbot = RAGChatbot(pdf_path, config)\n",
        "\n",
        "    print(\"\\nRAG Chatbot initialized! Type 'exit' to end the conversation.\")\n",
        "    print(\"This chatbot uses the following components:\")\n",
        "    print(\"1. Document Processing: Splits PDF into manageable chunks\")\n",
        "    print(\"2. Vector Database: Stores and retrieves relevant context\")\n",
        "    print(\"3. LLM Integration: Generates responses using Groq's Mixtral model\")\n",
        "    print(\"4. RAG Framework: Orchestrates the entire process\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(\"\\nChatbot:\", response[\"answer\"])\n",
        "        if response[\"sources\"]:\n",
        "            pass\n",
        "            # print(\"\\nSources used:\")\n",
        "            # print(response[\"sources\"][0])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "JdCDZyW9-rg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1txiszd-1zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq"
      ],
      "metadata": {
        "id": "RbYRdVcE-21m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "X-1LwxVC-9LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq()\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"llava-v1.5-7b-4096-preview\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://media.istockphoto.com/id/961805314/photo/a-new-beginning-into-a-sunny-future.jpg?s=1024x1024&w=is&k=20&c=Msk83herhD09mea1_XahyeL7yWMHkLZ4_Eq8laT59qM=\"}}\n",
        "            ]\n",
        "        },\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_tokens=1024,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "nfG20crk--JE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}